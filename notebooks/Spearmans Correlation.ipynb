{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e37212",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c23a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n",
      "warning in coregionalize: failed to import cython module: falling back to numpy\n",
      "warning in choleskies: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import GPy\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import norm, percentileofscore\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef72f8",
   "metadata": {},
   "source": [
    "# Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26713b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_meters_path = \"..\\\\data\\\\meters\\\\raw\\\\\"\n",
    "\n",
    "# files in directory\n",
    "files = glob(raw_meters_path + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7aa75ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>meter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                building_id  meter_reading         meter\n",
       "0  2016-01-01 00:00:00  Panther_office_Clementine            NaN  chilledwater\n",
       "1  2016-01-01 01:00:00  Panther_office_Clementine            NaN  chilledwater\n",
       "2  2016-01-01 02:00:00  Panther_office_Clementine            NaN  chilledwater\n",
       "3  2016-01-01 03:00:00  Panther_office_Clementine            NaN  chilledwater\n",
       "4  2016-01-01 04:00:00  Panther_office_Clementine            NaN  chilledwater"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfs = [] # empty list of the dataframes to create\n",
    "for file in files: # for each file in directory\n",
    "    meter_type = file.split(\"\\\\\")[4].split(\".\")[0] # meter_type to rename the value feature\n",
    "    meter = pd.read_csv(file) # load the dataset\n",
    "    meter = pd.melt(meter, id_vars = \"timestamp\", var_name = \"building_id\", value_name = \"meter_reading\") # melt dataset\n",
    "    meter[\"meter\"] = str(meter_type) # adds column with the meter type\n",
    "    dfs.append(meter) # append to list\n",
    "raw_data = pd.concat(dfs, axis=0, ignore_index=True) # concatenate all meter\n",
    "del(dfs, meter, file, files, meter_type)\n",
    "\n",
    "raw_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa402c59",
   "metadata": {},
   "source": [
    "# Load cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab035726",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_meters_path = \"..\\\\data\\\\meters\\\\cleaned\\\\\"\n",
    "\n",
    "# files in directory\n",
    "files = glob(cleaned_meters_path + \"*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87c0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>meter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                building_id  meter_reading  \\\n",
       "0  2016-01-01 00:00:00  Panther_office_Clementine            NaN   \n",
       "1  2016-01-01 01:00:00  Panther_office_Clementine            NaN   \n",
       "2  2016-01-01 02:00:00  Panther_office_Clementine            NaN   \n",
       "3  2016-01-01 03:00:00  Panther_office_Clementine            NaN   \n",
       "4  2016-01-01 04:00:00  Panther_office_Clementine            NaN   \n",
       "\n",
       "                  meter  \n",
       "0  chilledwater_cleaned  \n",
       "1  chilledwater_cleaned  \n",
       "2  chilledwater_cleaned  \n",
       "3  chilledwater_cleaned  \n",
       "4  chilledwater_cleaned  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [] # empty list of the dataframes to create\n",
    "for file in files: # for each file in directory\n",
    "    meter_type = file.split(\"\\\\\")[4].split(\".\")[0] # meter_type to rename the value feature\n",
    "    meter = pd.read_csv(file) # load the dataset\n",
    "    meter = pd.melt(meter, id_vars = \"timestamp\", var_name = \"building_id\", value_name = \"meter_reading\") # melt dataset\n",
    "    meter[\"meter\"] = str(meter_type) # adds column with the meter type\n",
    "    dfs.append(meter) # append to list\n",
    "complete_data_cleaned = pd.concat(dfs, axis=0, ignore_index=True) # concatenate all meter\n",
    "del(dfs, meter, file, files, meter_type)\n",
    "\n",
    "complete_data_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59aef182",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 409. MiB for an array with shape (53561832,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Add column indicating the year, month and dayOfTheWeek for that timestamp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m complete_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m complete_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\n\u001b[1;32m----> 8\u001b[0m complete_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m complete_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear\n\u001b[0;32m      9\u001b[0m complete_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m complete_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n\u001b[0;32m     10\u001b[0m complete_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdayOfWeek\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m complete_data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofweek\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4532\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4530\u001b[0m \u001b[38;5;66;03m# We should never get here with DataFrame value\u001b[39;00m\n\u001b[0;32m   4531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Series):\n\u001b[1;32m-> 4532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_reindex_for_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m   4535\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:10990\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  10986\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reindex_for_setitem\u001b[39m(value: DataFrame \u001b[38;5;241m|\u001b[39m Series, index: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[0;32m  10987\u001b[0m     \u001b[38;5;66;03m# reindex if necessary\u001b[39;00m\n\u001b[0;32m  10989\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mequals(index) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m> 10990\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10992\u001b[0m     \u001b[38;5;66;03m# GH#4107\u001b[39;00m\n\u001b[0;32m  10993\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 409. MiB for an array with shape (53561832,) and data type int64"
     ]
    }
   ],
   "source": [
    "# Note this cell might take some time to finish\n",
    "\n",
    "# Convert timestamp field from string into pd.datetime object\n",
    "complete_data_cleaned['timestamp'] = pd.to_datetime(complete_data_cleaned['timestamp'])\n",
    "\n",
    "# Add column indicating the year, month and dayOfTheWeek for that timestamp\n",
    "complete_data_cleaned['date'] = complete_data_cleaned['timestamp'].dt.date\n",
    "complete_data_cleaned['year'] = complete_data_cleaned['timestamp'].dt.year\n",
    "complete_data_cleaned['month'] = complete_data_cleaned['timestamp'].dt.month\n",
    "complete_data_cleaned['dayOfWeek'] = complete_data_cleaned['timestamp'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3034d",
   "metadata": {},
   "source": [
    "# Load benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c1c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_meters_path = \"..\\\\data\\\\\"\n",
    "\n",
    "# files in directory\n",
    "files = glob(cleaned_meters_path + \"*.csv\")\n",
    "\n",
    "\n",
    "benchmark = pd.read_csv(files[0]) # load the dataset\n",
    "\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad532379",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingNames = benchmark['name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc292e8",
   "metadata": {},
   "source": [
    "# Specify constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57926411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the first \n",
    "building_name = buildingNames[0]\n",
    "representative_df = complete_data_cleaned.loc[(complete_data_cleaned['building_id'] == building_name)\n",
    "                                               & (complete_data_cleaned['meter'] == 'electricity_cleaned')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479062fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.3\n",
    "DATAPOINS_PER_BUILDING_AND_METER_TYPE = representative_df.shape[0]\n",
    "SPLIT_INDEX = int(DATAPOINS_PER_BUILDING_AND_METER_TYPE * (1 - TRAIN_TEST_SPLIT))\n",
    "SPLIT_TIMESTAMP =  representative_df.iloc[SPLIT_INDEX]['timestamp']\n",
    "TRAIN_DIMENSIONS = 5\n",
    "SAMPLE_SIZE = 2000\n",
    "DATAPOINTS_ONE_WEEK = 24 * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e309dda",
   "metadata": {},
   "source": [
    "# Intermediate computations and ordering of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddae26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell might take some time to finish\n",
    "\n",
    "# Group the DataFrame by 'building_id' and 'meter_type'\n",
    "complete_data_grouped = complete_data_cleaned.groupby(['building_id', 'meter'])\n",
    "\n",
    "# Initialize an empty list to store the smaller DataFrames\n",
    "dfs = []\n",
    "df_keys = []\n",
    "\n",
    "# Iterate over the groups and create smaller DataFrames\n",
    "for group_key, group in complete_data_grouped:\n",
    "    df_keys.append(group_key)\n",
    "    dfs.append(group.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6735e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = np.array(df_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5dc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = []\n",
    "dfs_test = []\n",
    "\n",
    "for df in dfs: \n",
    "    \n",
    "    temp_train_df, temp_test_df = split_df_to_train_and_test(df, SPLIT_TIMESTAMP)\n",
    "    dfs_train.append(temp_train_df)\n",
    "    dfs_test.append(temp_test_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91100771",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Computation of Spearmas Rank Correlation\n",
    "@param  target: a pandas.dataframe (target) for wich we want to calculate the correlation matrix \n",
    "        possible_features: a list of pandas dataframes that are our features\n",
    "        keys: a helping list, that manages the building_names and keeps track of the order in our possible_features \n",
    "        name: the building_name of the current dataframe we are predicting\n",
    "@returns spearmans_matrix: a numpy.ndarray that contains the correlation value as first entry in each row and the corresponding p-value as a second element in each row. \n",
    "                            the order of stations in the matrix is consistent to the order in the dataset:\n",
    "'''\n",
    "def spearmansCorrelation (target, possible_features, name, keys = df_keys): \n",
    "    y_target = target['meter_reading']\n",
    "    spearmans_matrix = np.zeros((len(possible_features), 2))\n",
    "    for index, feature in enumerate(possible_features): \n",
    "        if ((target.shape[0] == feature.shape[0]) \n",
    "            and (keys[index] != (name, 'electricity_cleaned'))\n",
    "            and ((feature['meter_reading'].isnull().sum()/feature.shape[0]) < 0.7)):\n",
    "            \n",
    "            # if shapes match, we are not at our target dataframe and the corresponding feature has sufficient data\n",
    "            # we can compute the spearmans correlation\n",
    "            correlation, pval = stats.spearmanr(y_target, feature['meter_reading'], nan_policy='omit')\n",
    "            spearmans_matrix[index] = correlation, pval   \n",
    "\n",
    "        else: \n",
    "            spearmans_matrix[index] = 0,100 \n",
    "            continue\n",
    "            \n",
    "    return spearmans_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed any longer\n",
    "\n",
    "# For given dataset (list of pd.Dataframes) we compute a subsample of those dataframes such that we obtain a list of dfs\n",
    "# that do not fit the given building_name and are not of meter_type 'electricity_cleaned'\n",
    "# Also we obtain the index/position of the dataframe that matches the building_id and is of type electricity_cleaned\n",
    "def get_all_features(dataset, building_name):\n",
    "    key_to_exclude = [building_name, 'electricity_cleaned']\n",
    "    features = []\n",
    "    index_of_df = np.nan\n",
    "    for index, key in enumerate(group_keys):\n",
    "        # as the key composes of two elements, the buildling_id and the meter_type we compare two arrays of strings, therefore we need the .all() \n",
    "        if(key == key_to_exclude).all():\n",
    "            index_of_df = index\n",
    "            continue\n",
    "        features.append(dataset[index])\n",
    "            \n",
    "    return features, index_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be moved closer to the heading\n",
    "\n",
    "# Split a dataframe into train and test data according to a split_date\n",
    "\n",
    "\n",
    "def split_df_to_train_and_test(df, split_date):\n",
    "    df_train = df.loc[df['timestamp'] < split_date].copy().reset_index(drop=True) \n",
    "    df_test  = df.loc[df['timestamp'] >= split_date].copy().reset_index(drop=True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cb138",
   "metadata": {},
   "source": [
    "# Computation of the Spearmans correlation matrix, with all other possible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccbcc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingNames[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987599f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bear_utility_Sidney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\marku\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:4484: SpearmanRConstantInputWarning:An input array is constant; the correlation coefficient is not defined.\n",
      "  6%|████▉                                                                          | 1/16 [18:40<4:40:13, 1120.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cockatoo_religion_Diedre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████                                                                      | 2/16 [25:57<2:47:36, 718.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cockatoo_science_Rex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████                                                                 | 3/16 [41:37<2:57:35, 819.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eagle_education_Teresa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████                                                            | 4/16 [51:34<2:26:18, 731.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eagle_health_Lucinda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████████████████                                                       | 5/16 [59:00<1:55:16, 628.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fox_food_Francesco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████▎                                                | 6/16 [1:16:32<2:08:47, 772.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fox_parking_Tommie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|██████████████████████████████████▏                                           | 7/16 [1:31:27<2:01:52, 812.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gator_other_Gertrude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████                                       | 8/16 [1:38:36<1:32:04, 690.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hog_office_Bill\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████████████████████████▉                                  | 9/16 [1:46:21<1:12:20, 620.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hog_services_Kerrie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████▏                            | 10/16 [2:01:39<1:11:12, 712.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hog_warehouse_Porsha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████▉                        | 11/16 [2:15:51<1:02:54, 754.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamb_assembly_Bertie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████▎                   | 12/16 [2:28:22<50:14, 753.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamb_industrial_Carla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████████████████████████████████████▏              | 13/16 [2:45:54<42:12, 844.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peacock_lodging_Matthew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████▏         | 14/16 [3:01:51<29:16, 878.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rat_public_Loretta\n"
     ]
    }
   ],
   "source": [
    "correlation_matrices = []\n",
    "for name in tqdm(buildingNames[14:]): \n",
    "    print(name)\n",
    "    \n",
    "    # Get dataframe that matches our building_id\n",
    "    df = complete_data_cleaned.loc[(complete_data_cleaned['building_id']  == name) \n",
    "                                   & (complete_data_cleaned['meter'] == 'electricity_cleaned') ]\n",
    "    \n",
    "    # For later reconstruction we split the electricity meter reading into train and test set\n",
    "    y_train_df, y_test_df = split_df_to_train_and_test(df, SPLIT_TIMESTAMP)\n",
    "    \n",
    "    \n",
    "    # Compute the spearmans correlation between all those features in order to pick the most relevant ones\n",
    "    spearmans_matrix = spearmansCorrelation(y_train_df, dfs_train, name)\n",
    "    \n",
    "    \n",
    "    with open ('../data/correlation_matrix_{}.pkl'.format(name), 'wb') as outp:\n",
    "        pickle.dump(spearmans_matrix, outp, pickle.HIGHEST_PROTOCOL )\n",
    "    \n",
    "    correlation_matrices.append((spearmans_matrix, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open ('../data/correlation_matrices.pkl', 'wb') as outp:\n",
    "    pickle.dump(correlation_matrices, outp, pickle.HIGHEST_PROTOCOL )\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
