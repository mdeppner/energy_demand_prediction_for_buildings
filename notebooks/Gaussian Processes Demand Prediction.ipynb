{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977ccc6b",
   "metadata": {},
   "source": [
    "# Student Project - Machine Learning for Renewable Energy Systems\n",
    "## Coding Track: Forecasting Building Energy Data\n",
    "\n",
    "### Task: Predict demand of the next step, next day and next week of at least one building per category.\n",
    "\n",
    "\n",
    "\n",
    "This notebook was created as project in the course of the seminar [Machine Learning for Renewable Energy Systems](https://www.mlsustainableenergy.com/teaching/machine-learning-for-renewable-energy-systems/) in the reserach group [\n",
    "Machine Learning in Sustainable Energy Systems](https://www.mlsustainableenergy.com/) at the University of TÃ¼bingen. <br> \n",
    "The code and the forecasting has been carried out in this jupyter notebook and is the work of \n",
    "\n",
    "    Markus Deppner   4106780   markus.deppner@student.uni-tuebingen.de\n",
    "    \n",
    "This project works with data from [\"The Building Data Genome 2 (BDG2) Data-Set\"](https://github.com/buds-lab/building-data-genome-project-2) and aims to predict the energy demand of at least one building per category. To carry out this project I decided to train a Gaussian Process Regression Model for each individual building to forecast the demands of the next steps.  \n",
    "\n",
    "This notebook ist designed in the following structure:\n",
    "\n",
    "### Outline\n",
    "\n",
    "* [1. Setup and Imports](#setup_section)\n",
    "    - Import all necessary packages and define helper functions\n",
    "\n",
    "* [2. Loading Datasets](#loading_datasets)\n",
    "    - Load raw, cleaned and weather data\n",
    "    - Load benchmarks\n",
    "    \n",
    "* [3. Specification of Constants](#constants)\n",
    "    - Constants that are used throughout the project/notebook are defined in this section.\n",
    "\n",
    "* [4. Gaussian Process Regression](#gpr)\n",
    "    - [4.1 Helper Functions](#gpr_helper)\n",
    "    - [4.2 Intermediate Computations](#gpr_compute)\n",
    "        - Extract list of multiple grouped dataframes from complete dataframe\n",
    "        - Split dataframe into list of train and test sets\n",
    "\n",
    "* [5. Forecasting](#forecasting)\n",
    "    - The actual forecasting using Gaussian Process Regression\n",
    "    - Saving the files on the system\n",
    "    \n",
    "    \n",
    "* [6. Evaluation](#evaluation)\n",
    "    - Compare performance with benchmarks\n",
    "    - Plot results\n",
    "\n",
    "\n",
    "In addition to this notebook there is a second notebook which carries out the time consuming computation of the Spearman rank correlation matrix, for each building we want to forecast. It computes the correlation of the target dataframes to all other buildings and meter types. The correlation matrices are loaded into this notebook. Those buildings and metering types which show the highest correlation for the given target are used to train and predict the Gaussian Process Regression Mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953ff67",
   "metadata": {},
   "source": [
    "# <a name=\"setup_section\"></a> 1. Setup: Manage Installations Imports and Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a608b",
   "metadata": {},
   "source": [
    "During the setup phase of this project there have been problems with incompatibilities of numpy and\n",
    "[GPy](https://github.com/SheffieldML/GPy) which is the library used for the Gaussian Process Regression.\n",
    "In case you run into problems when executing the imports try to install the missing libraries via pip install. <br>\n",
    "In case you have problems importing GPy try to update numpy and GPy to the newest version with the following statements\n",
    "\n",
    "!pip install --upgrade GPy <br>\n",
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5efd8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n",
      "warning in coregionalize: failed to import cython module: falling back to numpy\n",
      "warning in choleskies: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import GPy\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import norm, percentileofscore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee91f4",
   "metadata": {},
   "source": [
    "# <a name=\"loading_datasets\"></a> 2. Load Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a60fec",
   "metadata": {},
   "source": [
    "This section loads the provided Data from the [\"The Building Data Genome 2 (BDG2) Data-Set\"](https://github.com/buds-lab/building-data-genome-project-2) and the benchmarks for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3380e0e",
   "metadata": {},
   "source": [
    "## Load metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad544a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metdata_path = \"..\\\\data\\\\metadata\\\\\"\n",
    "metadata = pd.read_csv(metdata_path + \"metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a1cd4",
   "metadata": {},
   "source": [
    "## Load weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52f7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data\n",
    "weather_path = \"..\\\\data\\\\weather\\\\\"\n",
    "weather = pd.read_csv(weather_path + \"weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d737cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp field from string into pd.datetime object\n",
    "weather['timestamp'] = pd.to_datetime(weather['timestamp'])\n",
    "\n",
    "# Add column indicating the year, month and dayOfTheWeek for that timestamp\n",
    "weather['date'] = weather['timestamp'].dt.date\n",
    "weather['month'] = weather['timestamp'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfd60c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site_id</th>\n",
       "      <th>airTemperature</th>\n",
       "      <th>cloudCoverage</th>\n",
       "      <th>dewTemperature</th>\n",
       "      <th>precipDepth1HR</th>\n",
       "      <th>precipDepth6HR</th>\n",
       "      <th>seaLvlPressure</th>\n",
       "      <th>windDirection</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>19.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>21.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>21.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1018.8</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>20.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1018.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>21.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  site_id  airTemperature  cloudCoverage  dewTemperature  \\\n",
       "0 2016-01-01 00:00:00  Panther            19.4            NaN            19.4   \n",
       "1 2016-01-01 01:00:00  Panther            21.1            6.0            21.1   \n",
       "2 2016-01-01 02:00:00  Panther            21.1            NaN            21.1   \n",
       "3 2016-01-01 03:00:00  Panther            20.6            NaN            20.0   \n",
       "4 2016-01-01 04:00:00  Panther            21.1            NaN            20.6   \n",
       "\n",
       "   precipDepth1HR  precipDepth6HR  seaLvlPressure  windDirection  windSpeed  \\\n",
       "0             0.0             NaN             NaN            0.0        0.0   \n",
       "1            -1.0             NaN          1019.4            0.0        0.0   \n",
       "2             0.0             NaN          1018.8          210.0        1.5   \n",
       "3             0.0             NaN          1018.1            0.0        0.0   \n",
       "4             0.0             NaN          1019.0          290.0        1.5   \n",
       "\n",
       "         date  month  \n",
       "0  2016-01-01      1  \n",
       "1  2016-01-01      1  \n",
       "2  2016-01-01      1  \n",
       "3  2016-01-01      1  \n",
       "4  2016-01-01      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94d826",
   "metadata": {},
   "source": [
    "## Load cleaned dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6090894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_meters_path = \"..\\\\data\\\\meters\\\\cleaned\\\\\"\n",
    "\n",
    "# files in directory\n",
    "files = glob(cleaned_meters_path + \"*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd253e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>meter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                building_id  meter_reading  \\\n",
       "0  2016-01-01 00:00:00  Panther_office_Clementine            NaN   \n",
       "1  2016-01-01 01:00:00  Panther_office_Clementine            NaN   \n",
       "2  2016-01-01 02:00:00  Panther_office_Clementine            NaN   \n",
       "3  2016-01-01 03:00:00  Panther_office_Clementine            NaN   \n",
       "4  2016-01-01 04:00:00  Panther_office_Clementine            NaN   \n",
       "\n",
       "                  meter  \n",
       "0  chilledwater_cleaned  \n",
       "1  chilledwater_cleaned  \n",
       "2  chilledwater_cleaned  \n",
       "3  chilledwater_cleaned  \n",
       "4  chilledwater_cleaned  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [] # empty list of the dataframes to create\n",
    "for file in files: # for each file in directory\n",
    "    meter_type = file.split(\"\\\\\")[4].split(\".\")[0] # meter_type to rename the value feature\n",
    "    meter = pd.read_csv(file) # load the dataset\n",
    "    meter = pd.melt(meter, id_vars = \"timestamp\", var_name = \"building_id\", value_name = \"meter_reading\") # melt dataset\n",
    "    meter[\"meter\"] = str(meter_type) # adds column with the meter type\n",
    "    dfs.append(meter) # append to list\n",
    "complete_data_cleaned = pd.concat(dfs, axis=0, ignore_index=True) # concatenate all meter\n",
    "del(dfs, meter, file, files, meter_type)\n",
    "\n",
    "complete_data_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a2f7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this cell might take some time to finish\n",
    "\n",
    "# Convert timestamp field from string into pd.datetime object\n",
    "complete_data_cleaned['timestamp'] = pd.to_datetime(complete_data_cleaned['timestamp'])\n",
    "\n",
    "# Add column indicating the year, month and dayOfTheWeek for that timestamp\n",
    "complete_data_cleaned['date'] = complete_data_cleaned['timestamp'].dt.date\n",
    "complete_data_cleaned['year'] = complete_data_cleaned['timestamp'].dt.year\n",
    "complete_data_cleaned['month'] = complete_data_cleaned['timestamp'].dt.month\n",
    "complete_data_cleaned['dayOfWeek'] = complete_data_cleaned['timestamp'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4e9e9",
   "metadata": {},
   "source": [
    "# Load benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56583e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>building_id</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>horizon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>utility</td>\n",
       "      <td>1.157131</td>\n",
       "      <td>0.846614</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>utility</td>\n",
       "      <td>1.255013</td>\n",
       "      <td>0.862390</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>utility</td>\n",
       "      <td>1.851878</td>\n",
       "      <td>1.167219</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cockatoo_religion_Diedre</td>\n",
       "      <td>religion</td>\n",
       "      <td>1.475301</td>\n",
       "      <td>1.018945</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cockatoo_religion_Diedre</td>\n",
       "      <td>religion</td>\n",
       "      <td>2.349360</td>\n",
       "      <td>1.820794</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cockatoo_religion_Diedre</td>\n",
       "      <td>religion</td>\n",
       "      <td>2.833513</td>\n",
       "      <td>1.958076</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cockatoo_science_Rex</td>\n",
       "      <td>science</td>\n",
       "      <td>7.304536</td>\n",
       "      <td>5.529282</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cockatoo_science_Rex</td>\n",
       "      <td>science</td>\n",
       "      <td>10.882962</td>\n",
       "      <td>7.975783</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cockatoo_science_Rex</td>\n",
       "      <td>science</td>\n",
       "      <td>12.667458</td>\n",
       "      <td>8.261340</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eagle_education_Teresa</td>\n",
       "      <td>education</td>\n",
       "      <td>8.286079</td>\n",
       "      <td>5.855556</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Eagle_education_Teresa</td>\n",
       "      <td>education</td>\n",
       "      <td>11.534440</td>\n",
       "      <td>8.819952</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Eagle_education_Teresa</td>\n",
       "      <td>education</td>\n",
       "      <td>14.939611</td>\n",
       "      <td>10.992661</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Eagle_health_Lucinda</td>\n",
       "      <td>health</td>\n",
       "      <td>24.377798</td>\n",
       "      <td>14.279867</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Eagle_health_Lucinda</td>\n",
       "      <td>health</td>\n",
       "      <td>40.084198</td>\n",
       "      <td>28.209270</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Eagle_health_Lucinda</td>\n",
       "      <td>health</td>\n",
       "      <td>50.877437</td>\n",
       "      <td>35.407537</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fox_food_Francesco</td>\n",
       "      <td>food</td>\n",
       "      <td>9.409997</td>\n",
       "      <td>6.518361</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Fox_food_Francesco</td>\n",
       "      <td>food</td>\n",
       "      <td>10.331829</td>\n",
       "      <td>7.536682</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fox_food_Francesco</td>\n",
       "      <td>food</td>\n",
       "      <td>18.896017</td>\n",
       "      <td>10.796031</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Fox_parking_Tommie</td>\n",
       "      <td>parking</td>\n",
       "      <td>2.536276</td>\n",
       "      <td>1.177474</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fox_parking_Tommie</td>\n",
       "      <td>parking</td>\n",
       "      <td>3.155471</td>\n",
       "      <td>1.812750</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fox_parking_Tommie</td>\n",
       "      <td>parking</td>\n",
       "      <td>3.463871</td>\n",
       "      <td>1.573460</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Gator_other_Gertrude</td>\n",
       "      <td>other</td>\n",
       "      <td>0.232520</td>\n",
       "      <td>0.051120</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Gator_other_Gertrude</td>\n",
       "      <td>other</td>\n",
       "      <td>1.070888</td>\n",
       "      <td>0.786288</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gator_other_Gertrude</td>\n",
       "      <td>other</td>\n",
       "      <td>1.404810</td>\n",
       "      <td>0.946556</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hog_office_Bill</td>\n",
       "      <td>office</td>\n",
       "      <td>18.614739</td>\n",
       "      <td>10.711040</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hog_office_Bill</td>\n",
       "      <td>office</td>\n",
       "      <td>46.933820</td>\n",
       "      <td>29.432126</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hog_office_Bill</td>\n",
       "      <td>office</td>\n",
       "      <td>60.590159</td>\n",
       "      <td>34.006334</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hog_services_Kerrie</td>\n",
       "      <td>services</td>\n",
       "      <td>2.075842</td>\n",
       "      <td>1.448992</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hog_services_Kerrie</td>\n",
       "      <td>services</td>\n",
       "      <td>3.061034</td>\n",
       "      <td>2.400821</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hog_services_Kerrie</td>\n",
       "      <td>services</td>\n",
       "      <td>3.818607</td>\n",
       "      <td>2.549373</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Hog_warehouse_Porsha</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>1.958354</td>\n",
       "      <td>0.945058</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Hog_warehouse_Porsha</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>2.141530</td>\n",
       "      <td>1.150929</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Hog_warehouse_Porsha</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>2.911756</td>\n",
       "      <td>1.397100</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lamb_assembly_Bertie</td>\n",
       "      <td>assembly</td>\n",
       "      <td>17.967060</td>\n",
       "      <td>10.985065</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Lamb_assembly_Bertie</td>\n",
       "      <td>assembly</td>\n",
       "      <td>28.582859</td>\n",
       "      <td>22.667979</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Lamb_assembly_Bertie</td>\n",
       "      <td>assembly</td>\n",
       "      <td>48.756165</td>\n",
       "      <td>28.608991</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Lamb_industrial_Carla</td>\n",
       "      <td>industrial</td>\n",
       "      <td>45.353234</td>\n",
       "      <td>30.321867</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Lamb_industrial_Carla</td>\n",
       "      <td>industrial</td>\n",
       "      <td>43.913538</td>\n",
       "      <td>31.891065</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Lamb_industrial_Carla</td>\n",
       "      <td>industrial</td>\n",
       "      <td>53.681570</td>\n",
       "      <td>26.884887</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Peacock_lodging_Matthew</td>\n",
       "      <td>lodging</td>\n",
       "      <td>3.862391</td>\n",
       "      <td>2.968328</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Peacock_lodging_Matthew</td>\n",
       "      <td>lodging</td>\n",
       "      <td>4.518313</td>\n",
       "      <td>3.510761</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Peacock_lodging_Matthew</td>\n",
       "      <td>lodging</td>\n",
       "      <td>8.428719</td>\n",
       "      <td>6.074870</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Rat_public_Loretta</td>\n",
       "      <td>public</td>\n",
       "      <td>2.925038</td>\n",
       "      <td>1.817659</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Rat_public_Loretta</td>\n",
       "      <td>public</td>\n",
       "      <td>9.891990</td>\n",
       "      <td>6.787990</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Rat_public_Loretta</td>\n",
       "      <td>public</td>\n",
       "      <td>16.948175</td>\n",
       "      <td>12.082286</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Wolf_retail_Marcella</td>\n",
       "      <td>retail</td>\n",
       "      <td>1.187043</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Wolf_retail_Marcella</td>\n",
       "      <td>retail</td>\n",
       "      <td>1.857042</td>\n",
       "      <td>1.375036</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wolf_retail_Marcella</td>\n",
       "      <td>retail</td>\n",
       "      <td>3.254573</td>\n",
       "      <td>2.155485</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name building_id       RMSE        MAE horizon\n",
       "0        Bear_utility_Sidney     utility   1.157131   0.846614  hourly\n",
       "1        Bear_utility_Sidney     utility   1.255013   0.862390   daily\n",
       "2        Bear_utility_Sidney     utility   1.851878   1.167219  weekly\n",
       "3   Cockatoo_religion_Diedre    religion   1.475301   1.018945  hourly\n",
       "4   Cockatoo_religion_Diedre    religion   2.349360   1.820794   daily\n",
       "5   Cockatoo_religion_Diedre    religion   2.833513   1.958076  weekly\n",
       "6       Cockatoo_science_Rex     science   7.304536   5.529282  hourly\n",
       "7       Cockatoo_science_Rex     science  10.882962   7.975783   daily\n",
       "8       Cockatoo_science_Rex     science  12.667458   8.261340  weekly\n",
       "9     Eagle_education_Teresa   education   8.286079   5.855556  hourly\n",
       "10    Eagle_education_Teresa   education  11.534440   8.819952   daily\n",
       "11    Eagle_education_Teresa   education  14.939611  10.992661  weekly\n",
       "12      Eagle_health_Lucinda      health  24.377798  14.279867  hourly\n",
       "13      Eagle_health_Lucinda      health  40.084198  28.209270   daily\n",
       "14      Eagle_health_Lucinda      health  50.877437  35.407537  weekly\n",
       "15        Fox_food_Francesco        food   9.409997   6.518361  hourly\n",
       "16        Fox_food_Francesco        food  10.331829   7.536682   daily\n",
       "17        Fox_food_Francesco        food  18.896017  10.796031  weekly\n",
       "18        Fox_parking_Tommie     parking   2.536276   1.177474  hourly\n",
       "19        Fox_parking_Tommie     parking   3.155471   1.812750   daily\n",
       "20        Fox_parking_Tommie     parking   3.463871   1.573460  weekly\n",
       "21      Gator_other_Gertrude       other   0.232520   0.051120  hourly\n",
       "22      Gator_other_Gertrude       other   1.070888   0.786288   daily\n",
       "23      Gator_other_Gertrude       other   1.404810   0.946556  weekly\n",
       "24           Hog_office_Bill      office  18.614739  10.711040  hourly\n",
       "25           Hog_office_Bill      office  46.933820  29.432126   daily\n",
       "26           Hog_office_Bill      office  60.590159  34.006334  weekly\n",
       "27       Hog_services_Kerrie    services   2.075842   1.448992  hourly\n",
       "28       Hog_services_Kerrie    services   3.061034   2.400821   daily\n",
       "29       Hog_services_Kerrie    services   3.818607   2.549373  weekly\n",
       "30      Hog_warehouse_Porsha   warehouse   1.958354   0.945058  hourly\n",
       "31      Hog_warehouse_Porsha   warehouse   2.141530   1.150929   daily\n",
       "32      Hog_warehouse_Porsha   warehouse   2.911756   1.397100  weekly\n",
       "33      Lamb_assembly_Bertie    assembly  17.967060  10.985065  hourly\n",
       "34      Lamb_assembly_Bertie    assembly  28.582859  22.667979   daily\n",
       "35      Lamb_assembly_Bertie    assembly  48.756165  28.608991  weekly\n",
       "36     Lamb_industrial_Carla  industrial  45.353234  30.321867  hourly\n",
       "37     Lamb_industrial_Carla  industrial  43.913538  31.891065   daily\n",
       "38     Lamb_industrial_Carla  industrial  53.681570  26.884887  weekly\n",
       "39   Peacock_lodging_Matthew     lodging   3.862391   2.968328  hourly\n",
       "40   Peacock_lodging_Matthew     lodging   4.518313   3.510761   daily\n",
       "41   Peacock_lodging_Matthew     lodging   8.428719   6.074870  weekly\n",
       "42        Rat_public_Loretta      public   2.925038   1.817659  hourly\n",
       "43        Rat_public_Loretta      public   9.891990   6.787990   daily\n",
       "44        Rat_public_Loretta      public  16.948175  12.082286  weekly\n",
       "45      Wolf_retail_Marcella      retail   1.187043   0.793767  hourly\n",
       "46      Wolf_retail_Marcella      retail   1.857042   1.375036   daily\n",
       "47      Wolf_retail_Marcella      retail   3.254573   2.155485  weekly"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_path = \"..\\\\data\\\\\"\n",
    "\n",
    "# files in directory\n",
    "files = glob(benchmark_path + \"*.csv\")\n",
    "\n",
    "\n",
    "benchmark = pd.read_csv(files[0]) # load the dataset\n",
    "\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a78fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of buildingNames that need to be predicted and for which a benchmark is provided\n",
    "buildingNames = benchmark['name'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f1a0e",
   "metadata": {},
   "source": [
    "#  <a name=\"constants\"></a> 3. Specify Constants\n",
    "\n",
    "Constants that are used throughout the project/notebook are defined in this section.\n",
    "In order to obtain some of the constants we need to load a representative dataframe of one building for one meter_type.\n",
    "In case to change some behaviour of the Gaussian Process ar the train and test data we need to modify the constants in this seciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "182d4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the dataset with the first name building_name from the benchmarks as a representative dataframe\n",
    "building_name = buildingNames[0]\n",
    "representative_df = complete_data_cleaned.loc[(complete_data_cleaned['building_id'] == building_name)\n",
    "                                               & (complete_data_cleaned['meter'] == 'electricity_cleaned')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24e3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of the test dataset the remaining percent will build the training dataset\n",
    "TRAIN_TEST_SPLIT = 0.3\n",
    "\n",
    "DATAPOINS_PER_BUILDING_AND_METER_TYPE = representative_df.shape[0]\n",
    "\n",
    "# Defines the index at which we split our dataset\n",
    "SPLIT_INDEX = int(DATAPOINS_PER_BUILDING_AND_METER_TYPE * (1 - TRAIN_TEST_SPLIT))\n",
    "\n",
    "# Defines the timestamp which is the timestamp at whcih we split into train and test set\n",
    "SPLIT_TIMESTAMP =  representative_df.iloc[SPLIT_INDEX]['timestamp']\n",
    "\n",
    "# Dimensions/ Number of correlated dataframes to include in the Gaussian Process Regression model\n",
    "GPR_DIMENSIONS = 5\n",
    "\n",
    "SAMPLE_SIZE = 2000\n",
    "DATAPOINTS_ONE_HOUR = 1\n",
    "DATAPOINTS_ONE_DAY = 24\n",
    "DATAPOINTS_ONE_WEEK = 24 * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469aace4",
   "metadata": {},
   "source": [
    "# <a name=\"gpr\"></a> 4. Gaussian Process Regression\n",
    "\n",
    "In this section we define all the necessary helper functions for Gaussian Process Regressiona and intermediate computations, such that we can iterate in the last section over all buildings we want to predict \n",
    "\n",
    "## <a name=\"gpr_helper\"></a> 4.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d986cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normalizes the data to a Gaussian distribution using quantiles.\n",
    "\"\"\"\n",
    "def normalize_to_gaussian(arr, mode=\"mean\"):\n",
    "    n = len(arr)\n",
    "    perc = percentileofscore\n",
    "    arr_ = arr.copy()[~np.isnan(arr)]\n",
    "    out = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if not np.isnan(arr[i]):\n",
    "            out[i] = norm.ppf(perc(arr_, arr[i], mode) / 100.)\n",
    "        else:\n",
    "            out[i] = np.nan\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65d5d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that transforms the Gaussian normalisation back to percentiles of scores\n",
    "'''\n",
    "def gaussian_to_cdf(arr):\n",
    "    n = len(arr)\n",
    "    out = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if not np.isnan(arr[i]):\n",
    "            out[i] = norm.cdf(arr[i])\n",
    "        else:\n",
    "            out[i] = np.nan\n",
    "            \n",
    "    out = out\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0725cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to compute the predictions which have been transformed to Gaussian distribution with the function \n",
    "def transform_into_original_range(arr, original):\n",
    "\n",
    "    \n",
    "    original_min = np.min(original)\n",
    "    original_max = np.max(original)\n",
    "    original_mean = np.nanmean(original) \n",
    "    original_std = np.nanstd(original)\n",
    "\n",
    "    # Calculate the range, mean, and standard deviation of the predictions\n",
    "    arr_range = np.max(arr) - np.min(arr)\n",
    "    arr_mean = np.mean(arr)\n",
    "    arr_std = np.std(arr)\n",
    "\n",
    "    # Rescale the predictions based on the original signal's range and statistics\n",
    "    rescaled = (\n",
    "        (arr - arr_mean) * (original_std / arr_std) + original_mean\n",
    "    )\n",
    "\n",
    "    # Adjust the rescaled predictions to fit within the original signal's range\n",
    "    min_diff = original_min - np.min(rescaled)\n",
    "    max_diff = np.max(rescaled) - original_max\n",
    "    if min_diff > 0:\n",
    "        rescaled -= min_diff\n",
    "    elif max_diff > 0:\n",
    "        rescaled -= max_diff\n",
    " \n",
    "    return rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43572cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Replaces missing values in the dataframe with the mean of the month for all years from which we have data\n",
    "@param dataframe\n",
    "@returns numpy array\n",
    "'''\n",
    "def averageNaNs (df, field):\n",
    "    mean_df = df.groupby(['month']).mean()\n",
    "    mean_df['meter_reading'].fillna(mean_df['meter_reading'].mean(), inplace=True)\n",
    "    if (len(mean_df.index) == 12):\n",
    "        averaged_mean = df[field].copy().fillna(\n",
    "                                    df['month'].map({1: mean_df[field][1] , 2: mean_df[field][2], 3:mean_df[field][3],\n",
    "                                                     4: mean_df[field][4] , 5: mean_df[field][5], 6:mean_df[field][6], \n",
    "                                                     7: mean_df[field][7] , 8: mean_df[field][8], 9:mean_df[field][9],\n",
    "                                                     10: mean_df[field][10] , 11: mean_df[field][11], 12:mean_df[field][12]}))\n",
    "    else :\n",
    "        averaged_mean = df[field].copy().fillna(\n",
    "                                    df['month'].map({5: mean_df[field][5], 6:mean_df[field][6], \n",
    "                                                     7: mean_df[field][7] , 8: mean_df[field][8], 9: mean_df[field][9],\n",
    "                                                     10: mean_df[field][10] , 11: mean_df[field][11], 12: mean_df[field][12]}))\n",
    "    averaged_numpy = averaged_mean.to_numpy()\n",
    "    return averaged_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "242e16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train a Gaussian Process model based on the training data\n",
    "@param two numpy arrays of same dimensionality\n",
    "@returns numpy array with lenght NUMBER_OF_DIMENSIONS, containing the indices of the most correlated stations\n",
    "'''\n",
    "def trainGP (X_train, Y_train):\n",
    "    \n",
    "    # Shape of training data needs to be consistent \n",
    "    assert X_train.shape[0] == Y_train.shape[0]\n",
    "    \n",
    "    kernel = GPy.kern.RBF(input_dim=X_train.shape[1])\n",
    "    model = GPy.models.GPRegression(X_train, Y_train, kernel)\n",
    "    model.optimize(messages=True)\n",
    "    model.optimize_restarts(num_restarts=5)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb57083",
   "metadata": {},
   "source": [
    "## Iteration over all buildings we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "359bdebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Probably not needed any longer!\n",
    "\n",
    "\n",
    "# For given dataset (list of pd.Dataframes) we compute a subsample of those dataframes such that we obtain a list of dfs\n",
    "# that do not fit the given building_name and are not of meter_type 'electricity_cleaned'\n",
    "# Also we obtain the index/position of the dataframe that matches the building_id and is of type electricity_cleaned\n",
    "def get_all_features(dataset, building_name):\n",
    "    key_to_exclude = [building_name, 'electricity_cleaned']\n",
    "    features = []\n",
    "    index_of_df = np.nan\n",
    "    for index, key in enumerate(group_keys):\n",
    "        # as the key composes of two elements, the buildling_id and the meter_type we compare two arrays of strings, therefore we need the .all() \n",
    "        if(key == key_to_exclude).all():\n",
    "            index_of_df = index\n",
    "            continue\n",
    "        features.append(dataset[index])\n",
    "            \n",
    "    return features, index_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f356dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataframe into train and test data according to a split_date\n",
    "\n",
    "def split_df_to_train_and_test(df, split_date):\n",
    "    df_train = df.loc[df['timestamp'] < split_date].copy().reset_index(drop=True) \n",
    "    df_test  = df.loc[df['timestamp'] >= split_date].copy().reset_index(drop=True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def44fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def sort_according_to_highest_correlation(spearmans_matrix):\n",
    "    correlation_values = spearmans_matrix[:,0]\n",
    "    clean_correlation_values = np.nan_to_num(correlation_values, 0)\n",
    "    clean_abs_correlation_values = np.abs(clean_correlation_values)\n",
    "    sorted_indices = np.argsort(clean_abs_correlation_values)[::-1]\n",
    "    \n",
    "    return sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ac7af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a dataset (list of pd.Dataframes) and a list of sorted indice\n",
    "def transform_highest_correlated_dfs_to_gpr_data(list_of_dfs ,sorted_indices):\n",
    "    transformed_features = []\n",
    "    for ind in sorted_indices:\n",
    "        \n",
    "        if len(transformed_features) == GPR_DIMENSIONS:\n",
    "            break\n",
    "            \n",
    "        temp_df = list_of_dfs[ind]\n",
    "        temp_df_train, temp_df_test = split_df_to_train_and_test(temp_df, SPLIT_TIMESTAMP)\n",
    "        \n",
    "        if ((compute_percentage_of_nan_values(temp_df_train) > 0.3)\n",
    "            or (compute_percentage_of_nan_values(temp_df_test)> 0.3)):\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        avg_temp_arr = averageNaNs(temp_df, 'meter_reading')\n",
    "        gauss_arr = normalize_to_gaussian(avg_temp_arr)\n",
    "        transformed_features.append(gauss_arr)\n",
    "    \n",
    "    return np.column_stack((transformed_features))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f2776f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentage_of_nan_values(df):\n",
    "    rows = df.shape[0]\n",
    "    nans = np.isnan(df['meter_reading']).sum()\n",
    "    \n",
    "    return nans/rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6c3c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X_train, y_train, num_samples=SAMPLE_SIZE, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    indices = np.random.choice(len(X_train), size=num_samples, replace=False)\n",
    "    X_samples = X_train[indices]\n",
    "    y_samples = y_train[indices]\n",
    "\n",
    "    return X_samples, y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "328f4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spearmans_matrix (name): \n",
    "    with open('../data/correlations/correlation_matrix_{}.pkl'.format(name), 'rb') as inp:\n",
    "        correlation_matrix = pickle.load(inp)\n",
    "        \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a188709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gpr_predictions (mean, var, name):\n",
    "    \n",
    "    with open ('../data/predictions/mean_{}.pkl'.format(name), 'wb') as outp:\n",
    "        pickle.dump(mean, outp, pickle.HIGHEST_PROTOCOL )\n",
    "        \n",
    "    with open ('../data/predictions/var_{}.pkl'.format(name), 'wb') as outp:\n",
    "        pickle.dump(var, outp, pickle.HIGHEST_PROTOCOL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cf9dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transformed_predictions (prediction_transformed, variance_transformed, original_transformed, name):\n",
    "    with open ('../data/predictions/transformed/mean_{}.pkl'.format(name), 'wb') as outp:\n",
    "        pickle.dump(prediction_transformed, outp, pickle.HIGHEST_PROTOCOL )\n",
    "        \n",
    "    with open ('../data/predictions/transformed/var_{}.pkl'.format(name), 'wb') as outp:\n",
    "        pickle.dump(variance_transformed, outp, pickle.HIGHEST_PROTOCOL )\n",
    "        \n",
    "    with open ('../data/predictions/transformed/orig_{}.pkl'.format(name), 'wb') as outp:\n",
    "        pickle.dump(original_transformed, outp, pickle.HIGHEST_PROTOCOL )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249da91",
   "metadata": {},
   "source": [
    "## Error computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c962fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_metrics(y_target, prediction):\n",
    "    \n",
    "    assert (y_target.shape == prediction.shape)\n",
    "    \n",
    "    if np.isnan(y_target).any():\n",
    "        mask = ~np.isnan(y_target)\n",
    "        mse = mean_squared_error(y_target[mask], prediction[mask])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_target[mask], prediction[mask])\n",
    "    \n",
    "    else: \n",
    "        mse = mean_squared_error(y_target, prediction)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_target, prediction)\n",
    "\n",
    "    \n",
    "    return rmse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71e49421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_errors(rmse, mae, time_horizon, name):\n",
    "    with open ('../data/errors/{}/rmse_{}.pkl'.format(time_horizon, name), 'wb') as outp:\n",
    "        pickle.dump(rmse, outp, pickle.HIGHEST_PROTOCOL )\n",
    "    \n",
    "    with open ('../data/errors/{}/mae_{}.pkl'.format(time_horizon, name), 'wb') as outp:\n",
    "        pickle.dump(mae, outp, pickle.HIGHEST_PROTOCOL )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1b68e",
   "metadata": {},
   "source": [
    "## <a name=\"gpr_intermediate\"></a>  4.2  Intermediate Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77cd82e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cell might take some time to finish\n",
    "\n",
    "# Group the DataFrame by 'building_id' and 'meter_type'\n",
    "complete_data_grouped = complete_data_cleaned.groupby(['building_id', 'meter'])\n",
    "\n",
    "# Initialize an empty list to store the smaller DataFrames\n",
    "dfs = []\n",
    "df_keys = []\n",
    "\n",
    "# Iterate over the groups and create smaller DataFrames\n",
    "for group_key, group in complete_data_grouped:\n",
    "    df_keys.append(group_key)\n",
    "    dfs.append(group.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bccea9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = np.array(df_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dffbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = []\n",
    "dfs_test = []\n",
    "\n",
    "for df in dfs: \n",
    "    \n",
    "    temp_train_df, temp_test_df = split_df_to_train_and_test(df, SPLIT_TIMESTAMP)\n",
    "    dfs_train.append(temp_train_df)\n",
    "    dfs_test.append(temp_test_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf359cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af99180",
   "metadata": {},
   "source": [
    "# <a name=\"forecasting\"></a> 5. Electricity Demand Forecasting with Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0407fb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89603889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hog_warehouse_Porsha\n",
      "Load spearmans_matrix\n",
      "transforming highest correlated dfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453615926ef24d458b2c99bee20b04c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 1168.5492001252394\n",
      "Optimization restart 2/5, f = 1168.549200125819\n",
      "Optimization restart 3/5, f = 1168.5492001263333\n",
      "Optimization restart 4/5, f = 1168.549200128607\n",
      "Optimization restart 5/5, f = 1168.549200125381\n",
      "Compute predictions ...\n",
      "transform predictions back to original data range\n",
      "Compute errors ...\n",
      "Hourly\n",
      "Daily\n",
      "Weekly\n",
      "Lamb_assembly_Bertie\n",
      "Load spearmans_matrix\n",
      "transforming highest correlated dfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a16e3e4a9c45919530799318a7fd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 787.4777904227038\n",
      "Optimization restart 2/5, f = 787.4777904167233\n",
      "Optimization restart 3/5, f = 787.4777904167131\n",
      "Optimization restart 4/5, f = 787.4777904176793\n",
      "Optimization restart 5/5, f = 787.4777904167418\n",
      "Compute predictions ...\n",
      "transform predictions back to original data range\n",
      "Compute errors ...\n",
      "Hourly\n",
      "Daily\n",
      "Weekly\n",
      "Lamb_industrial_Carla\n",
      "Load spearmans_matrix\n",
      "transforming highest correlated dfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9cd561ae54461390a04ad7958aaa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 1179.6575131798922\n",
      "Optimization restart 2/5, f = 1179.6575131798636\n",
      "Optimization restart 3/5, f = 1179.657513197109\n",
      "Optimization restart 4/5, f = 1179.657513179825\n",
      "Optimization restart 5/5, f = 1179.657513184495\n",
      "Compute predictions ...\n",
      "transform predictions back to original data range\n",
      "Compute errors ...\n",
      "Hourly\n",
      "Daily\n",
      "Weekly\n",
      "Peacock_lodging_Matthew\n",
      "Load spearmans_matrix\n",
      "transforming highest correlated dfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2e6ad711d844d885504d88956d5fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 2277.5764930610844\n",
      "Optimization restart 2/5, f = 2277.5764930542837\n",
      "Optimization restart 3/5, f = 2277.5764930608125\n",
      "Optimization restart 4/5, f = 2277.576493054614\n",
      "Optimization restart 5/5, f = 2277.5764930614664\n",
      "Compute predictions ...\n",
      "transform predictions back to original data range\n",
      "Compute errors ...\n",
      "Hourly\n",
      "Daily\n",
      "Weekly\n",
      "Rat_public_Loretta\n",
      "Load spearmans_matrix\n",
      "transforming highest correlated dfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69692c1239d4ef1af31d4f7404dce25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 1269.5722341590613\n",
      "Optimization restart 2/5, f = 1269.5722341686658\n",
      "Optimization restart 3/5, f = 1269.572234174639\n",
      "Optimization restart 4/5, f = 1269.5722341591154\n",
      "Optimization restart 5/5, f = 1269.572234163199\n",
      "Compute predictions ...\n",
      "transform predictions back to original data range\n",
      "Compute errors ...\n",
      "Hourly\n",
      "Daily\n",
      "Weekly\n",
      "Wolf_retail_Marcella\n",
      "Load spearmans_matrix\n",
      "transforming highest correlated dfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n",
      " C:\\Users\\marku\\AppData\\Local\\Temp\\ipykernel_22800\\2278653229.py:5: RuntimeWarning:invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2782400529f94b7bb29db71ad06fd658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 1824.3329968281982\n",
      "Optimization restart 2/5, f = 1824.3329968397754\n",
      "Optimization restart 3/5, f = 1824.3329968662001\n",
      "Optimization restart 4/5, f = 1824.3329968406506\n",
      "Optimization restart 5/5, f = 1824.332996828664\n",
      "Compute predictions ...\n",
      "transform predictions back to original data range\n",
      "Compute errors ...\n",
      "Hourly\n",
      "Daily\n",
      "Weekly\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"401ca44e-99e1-403c-9ccc-a19845642cc1\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"401ca44e-99e1-403c-9ccc-a19845642cc1\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "\n",
    "for name in buildingNames: \n",
    "    print(name)\n",
    "    \n",
    "    # Get dataframe that matches our building_id\n",
    "    df = complete_data_cleaned.loc[complete_data_cleaned['building_id']  == name]\n",
    "    \n",
    "    # For later reconstruction we split the electricity meter reading into train and test set\n",
    "    y_train_df, y_test_df = split_df_to_train_and_test(df, SPLIT_TIMESTAMP)\n",
    "    \n",
    "    \n",
    "    print(\"Load spearmans_matrix\")\n",
    "    spearmans_matrix = load_spearmans_matrix(name)\n",
    "    \n",
    "    \n",
    "    # Sort according to highest correlation\n",
    "    sorted_indices = sort_according_to_highest_correlation(spearmans_matrix)\n",
    "    \n",
    "    \n",
    "    print(\"transforming highest correlated dfs\")\n",
    "   \n",
    "    # Bring data into format to fit as GP Regression input (normal distribution)\n",
    "    X_train = transform_highest_correlated_dfs_to_gpr_data(dfs_train, sorted_indices)\n",
    "    X_test = transform_highest_correlated_dfs_to_gpr_data(dfs_test, sorted_indices)\n",
    "  \n",
    "    # ALso transform target data\n",
    "    Y_train = normalize_to_gaussian(averageNaNs(y_train_df, 'meter_reading'))\n",
    "    Y_test = normalize_to_gaussian(averageNaNs(y_test_df, 'meter_reading'))\n",
    " \n",
    "    \n",
    "    # Randomly sample from the datapoints, as we cannot train on the whole dimensions\n",
    "    sampled_X_train, sampled_Y_train = random_sampling(X_train, Y_train)\n",
    "            \n",
    "    \n",
    "    print(\"Training model ....\")\n",
    "    # Train the model \n",
    "    model = trainGP (sampled_X_train, sampled_Y_train.reshape(-1,1))\n",
    "    \n",
    "    print(\"Compute predictions ...\")\n",
    "    # Make predictions with the trained model for one week\n",
    "    mean, var = model.predict(X_test[0:DATAPOINTS_ONE_WEEK])\n",
    "    \n",
    "    save_gpr_predictions(mean, var, name)\n",
    "    \n",
    "    \n",
    "    print(\"transform predictions back to original data range\")\n",
    "    # Transform the predictions which take place in the gaussian domain back to the original data range\n",
    "    prediction_transformed = transform_into_original_range(mean.flatten(), y_train_df['meter_reading'])\n",
    "    variance_transformed = transform_into_original_range(var.flatten(), y_train_df['meter_reading'])\n",
    "    original_test_transformed = transform_into_original_range(y_test_df['meter_reading'][:DATAPOINTS_ONE_WEEK], y_train_df['meter_reading'])\n",
    "    \n",
    "    save_transformed_predictions(prediction_transformed, variance_transformed, original_test_transformed, name)\n",
    "\n",
    "    print(\"Compute errors ...\")\n",
    "    print(\"Hourly\")\n",
    "    #Compute the error values for each timespan hour, day, week\n",
    "    rmse_hour, mae_hour = compute_error_metrics(y_test_df['meter_reading'][:DATAPOINTS_ONE_HOUR].to_numpy(), \n",
    "                                                          prediction_transformed[:DATAPOINTS_ONE_HOUR])\n",
    "    save_errors(rmse_hour, mae_hour, 'hour', name)\n",
    "    \n",
    "    print(\"Daily\")\n",
    "    rmse_day, mae_day = compute_error_metrics(y_test_df['meter_reading'][:DATAPOINTS_ONE_DAY].to_numpy(), \n",
    "                                                       prediction_transformed[:DATAPOINTS_ONE_DAY])\n",
    "    save_errors(rmse_day, mae_day, 'day', name)\n",
    "    \n",
    "    print(\"Weekly\")\n",
    "    rmse_week, mae_week = compute_error_metrics(y_test_df['meter_reading'][:DATAPOINTS_ONE_WEEK].to_numpy(), \n",
    "                                                          prediction_transformed[:DATAPOINTS_ONE_WEEK])\n",
    "\n",
    "    save_errors(rmse_week, mae_week, 'week', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d07e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a28550",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b67ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0377da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca892ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(0, np.size(mean), np.size(mean))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Energy_consumption')\n",
    "\n",
    "plt.plot(x, mean, label='Prediction')\n",
    "plt.plot(x, test_meter[0:200], label='Ground truth')\n",
    "plt.fill_between(x, (mean - var).flatten(), (mean + var).flatten(), color='lightblue', alpha=0.5, label='Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f01bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca83ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c1c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84b0f5a9",
   "metadata": {},
   "source": [
    "## Specify and Train Gaussian Process model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba1404",
   "metadata": {},
   "source": [
    "###### Plot the results given driectly from the GP\n",
    "\n",
    "Note as we previously transformed the data to be normal distributed, the data we obtain here is as well transformed <br>\n",
    "In order to reconstruct the original data we need to transform it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dd0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x = np.linspace(0, np.size(mean), np.size(mean))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Energy_consumption')\n",
    "\n",
    "plt.plot(x, mean, label='Prediction')\n",
    "plt.plot(x, test_meter[0:200], label='Ground truth')\n",
    "plt.fill_between(x, (mean - var).flatten(), (mean + var).flatten(), color='lightblue', alpha=0.5, label='Variance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad919d",
   "metadata": {},
   "source": [
    "## Convert the results back and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beee30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "prediction_transformed = transform_into_original_range(mean.flatten(), original_data_train)\n",
    "variance_transformed = transform_into_original_range(var.flatten(), original_data_train)\n",
    "original_test_transformed = transform_into_original_range(original_data_test[0:200], original_data_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27521de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x = np.linspace(0, np.size(prediction_transformed), np.size(prediction_transformed))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Energy_consumption')\n",
    "\n",
    "plt.plot(x, prediction_transformed, label='Prediction')\n",
    "plt.plot(x, original_data_test[0:200], label='Ground truth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "direction": "ltr",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
