{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e953ff67",
   "metadata": {},
   "source": [
    "# <a name=\"setup_section\"></a> 1. Setup: Manage Installations Imports and Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bf2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529ddd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a24416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                             | 10/1000 [00:00<00:00, 9946.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 10)\n",
      "(1, 11)\n",
      "(2, 12)\n",
      "(3, 13)\n",
      "(4, 14)\n",
      "(5, 15)\n",
      "(6, 16)\n",
      "(7, 17)\n",
      "(8, 18)\n",
      "(9, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(tqdm(np.arange(10, 20))):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b593abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade GPy\n",
    "#!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5efd8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n",
      "warning in coregionalize: failed to import cython module: falling back to numpy\n",
      "warning in choleskies: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import GPy\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import norm, percentileofscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee91f4",
   "metadata": {},
   "source": [
    "# <a name=\"loading_datasets\"></a> 2. Loading Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3380e0e",
   "metadata": {},
   "source": [
    "## Load metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad544a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metdata_path = \"..\\\\data\\\\metadata\\\\\"\n",
    "metadata = pd.read_csv(metdata_path + \"metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a1cd4",
   "metadata": {},
   "source": [
    "## Load weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52f7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data\n",
    "weather_path = \"..\\\\data\\\\weather\\\\\"\n",
    "weather = pd.read_csv(weather_path + \"weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d737cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp field from string into pd.datetime object\n",
    "weather['timestamp'] = pd.to_datetime(weather['timestamp'])\n",
    "\n",
    "# Add column indicating the year, month and dayOfTheWeek for that timestamp\n",
    "weather['date'] = weather['timestamp'].dt.date\n",
    "weather['month'] = weather['timestamp'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfd60c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site_id</th>\n",
       "      <th>airTemperature</th>\n",
       "      <th>cloudCoverage</th>\n",
       "      <th>dewTemperature</th>\n",
       "      <th>precipDepth1HR</th>\n",
       "      <th>precipDepth6HR</th>\n",
       "      <th>seaLvlPressure</th>\n",
       "      <th>windDirection</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>19.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>21.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>21.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1018.8</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>20.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1018.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>Panther</td>\n",
       "      <td>21.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  site_id  airTemperature  cloudCoverage  dewTemperature  \\\n",
       "0 2016-01-01 00:00:00  Panther            19.4            NaN            19.4   \n",
       "1 2016-01-01 01:00:00  Panther            21.1            6.0            21.1   \n",
       "2 2016-01-01 02:00:00  Panther            21.1            NaN            21.1   \n",
       "3 2016-01-01 03:00:00  Panther            20.6            NaN            20.0   \n",
       "4 2016-01-01 04:00:00  Panther            21.1            NaN            20.6   \n",
       "\n",
       "   precipDepth1HR  precipDepth6HR  seaLvlPressure  windDirection  windSpeed  \\\n",
       "0             0.0             NaN             NaN            0.0        0.0   \n",
       "1            -1.0             NaN          1019.4            0.0        0.0   \n",
       "2             0.0             NaN          1018.8          210.0        1.5   \n",
       "3             0.0             NaN          1018.1            0.0        0.0   \n",
       "4             0.0             NaN          1019.0          290.0        1.5   \n",
       "\n",
       "         date  month  \n",
       "0  2016-01-01      1  \n",
       "1  2016-01-01      1  \n",
       "2  2016-01-01      1  \n",
       "3  2016-01-01      1  \n",
       "4  2016-01-01      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281d413",
   "metadata": {},
   "source": [
    "## Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c570a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_meters_path = \"..\\\\data\\\\meters\\\\raw\\\\\"\n",
    "\n",
    "# files in directory\n",
    "files = glob(raw_meters_path + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f621b97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>meter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                building_id  meter_reading         meter\n",
       "0  2016-01-01 00:00:00  Panther_office_Clementine            NaN  chilledwater\n",
       "1  2016-01-01 01:00:00  Panther_office_Clementine            NaN  chilledwater\n",
       "2  2016-01-01 02:00:00  Panther_office_Clementine            NaN  chilledwater\n",
       "3  2016-01-01 03:00:00  Panther_office_Clementine            NaN  chilledwater\n",
       "4  2016-01-01 04:00:00  Panther_office_Clementine            NaN  chilledwater"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfs = [] # empty list of the dataframes to create\n",
    "for file in files: # for each file in directory\n",
    "    meter_type = file.split(\"\\\\\")[4].split(\".\")[0] # meter_type to rename the value feature\n",
    "    meter = pd.read_csv(file) # load the dataset\n",
    "    meter = pd.melt(meter, id_vars = \"timestamp\", var_name = \"building_id\", value_name = \"meter_reading\") # melt dataset\n",
    "    meter[\"meter\"] = str(meter_type) # adds column with the meter type\n",
    "    dfs.append(meter) # append to list\n",
    "raw_data = pd.concat(dfs, axis=0, ignore_index=True) # concatenate all meter\n",
    "del(dfs, meter, file, files, meter_type)\n",
    "\n",
    "raw_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94d826",
   "metadata": {},
   "source": [
    "## Load cleaned dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6090894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_meters_path = \"..\\\\data\\\\meters\\\\cleaned\\\\\"\n",
    "\n",
    "# files in directory\n",
    "files = glob(cleaned_meters_path + \"*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd253e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>meter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>Panther_office_Clementine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chilledwater_cleaned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                building_id  meter_reading  \\\n",
       "0  2016-01-01 00:00:00  Panther_office_Clementine            NaN   \n",
       "1  2016-01-01 01:00:00  Panther_office_Clementine            NaN   \n",
       "2  2016-01-01 02:00:00  Panther_office_Clementine            NaN   \n",
       "3  2016-01-01 03:00:00  Panther_office_Clementine            NaN   \n",
       "4  2016-01-01 04:00:00  Panther_office_Clementine            NaN   \n",
       "\n",
       "                  meter  \n",
       "0  chilledwater_cleaned  \n",
       "1  chilledwater_cleaned  \n",
       "2  chilledwater_cleaned  \n",
       "3  chilledwater_cleaned  \n",
       "4  chilledwater_cleaned  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [] # empty list of the dataframes to create\n",
    "for file in files: # for each file in directory\n",
    "    meter_type = file.split(\"\\\\\")[4].split(\".\")[0] # meter_type to rename the value feature\n",
    "    meter = pd.read_csv(file) # load the dataset\n",
    "    meter = pd.melt(meter, id_vars = \"timestamp\", var_name = \"building_id\", value_name = \"meter_reading\") # melt dataset\n",
    "    meter[\"meter\"] = str(meter_type) # adds column with the meter type\n",
    "    dfs.append(meter) # append to list\n",
    "complete_data_cleaned = pd.concat(dfs, axis=0, ignore_index=True) # concatenate all meter\n",
    "del(dfs, meter, file, files, meter_type)\n",
    "\n",
    "complete_data_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a2f7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this cell might take some time to finish\n",
    "\n",
    "# Convert timestamp field from string into pd.datetime object\n",
    "complete_data_cleaned['timestamp'] = pd.to_datetime(complete_data_cleaned['timestamp'])\n",
    "\n",
    "# Add column indicating the year, month and dayOfTheWeek for that timestamp\n",
    "complete_data_cleaned['date'] = complete_data_cleaned['timestamp'].dt.date\n",
    "complete_data_cleaned['year'] = complete_data_cleaned['timestamp'].dt.year\n",
    "complete_data_cleaned['month'] = complete_data_cleaned['timestamp'].dt.month\n",
    "complete_data_cleaned['dayOfWeek'] = complete_data_cleaned['timestamp'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecb4a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meter_types = complete_data_cleaned['meter'].unique()\n",
    "other_meter_types = meter_types.tolist().remove('electricity_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4e9e9",
   "metadata": {},
   "source": [
    "# Load benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56583e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>building_id</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>horizon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>utility</td>\n",
       "      <td>1.157131</td>\n",
       "      <td>0.846614</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>utility</td>\n",
       "      <td>1.255013</td>\n",
       "      <td>0.862390</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>utility</td>\n",
       "      <td>1.851878</td>\n",
       "      <td>1.167219</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cockatoo_religion_Diedre</td>\n",
       "      <td>religion</td>\n",
       "      <td>1.475301</td>\n",
       "      <td>1.018945</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cockatoo_religion_Diedre</td>\n",
       "      <td>religion</td>\n",
       "      <td>2.349360</td>\n",
       "      <td>1.820794</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cockatoo_religion_Diedre</td>\n",
       "      <td>religion</td>\n",
       "      <td>2.833513</td>\n",
       "      <td>1.958076</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cockatoo_science_Rex</td>\n",
       "      <td>science</td>\n",
       "      <td>7.304536</td>\n",
       "      <td>5.529282</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cockatoo_science_Rex</td>\n",
       "      <td>science</td>\n",
       "      <td>10.882962</td>\n",
       "      <td>7.975783</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cockatoo_science_Rex</td>\n",
       "      <td>science</td>\n",
       "      <td>12.667458</td>\n",
       "      <td>8.261340</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eagle_education_Teresa</td>\n",
       "      <td>education</td>\n",
       "      <td>8.286079</td>\n",
       "      <td>5.855556</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Eagle_education_Teresa</td>\n",
       "      <td>education</td>\n",
       "      <td>11.534440</td>\n",
       "      <td>8.819952</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Eagle_education_Teresa</td>\n",
       "      <td>education</td>\n",
       "      <td>14.939611</td>\n",
       "      <td>10.992661</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Eagle_health_Lucinda</td>\n",
       "      <td>health</td>\n",
       "      <td>24.377798</td>\n",
       "      <td>14.279867</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Eagle_health_Lucinda</td>\n",
       "      <td>health</td>\n",
       "      <td>40.084198</td>\n",
       "      <td>28.209270</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Eagle_health_Lucinda</td>\n",
       "      <td>health</td>\n",
       "      <td>50.877437</td>\n",
       "      <td>35.407537</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fox_food_Francesco</td>\n",
       "      <td>food</td>\n",
       "      <td>9.409997</td>\n",
       "      <td>6.518361</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Fox_food_Francesco</td>\n",
       "      <td>food</td>\n",
       "      <td>10.331829</td>\n",
       "      <td>7.536682</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fox_food_Francesco</td>\n",
       "      <td>food</td>\n",
       "      <td>18.896017</td>\n",
       "      <td>10.796031</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Fox_parking_Tommie</td>\n",
       "      <td>parking</td>\n",
       "      <td>2.536276</td>\n",
       "      <td>1.177474</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fox_parking_Tommie</td>\n",
       "      <td>parking</td>\n",
       "      <td>3.155471</td>\n",
       "      <td>1.812750</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fox_parking_Tommie</td>\n",
       "      <td>parking</td>\n",
       "      <td>3.463871</td>\n",
       "      <td>1.573460</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Gator_other_Gertrude</td>\n",
       "      <td>other</td>\n",
       "      <td>0.232520</td>\n",
       "      <td>0.051120</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Gator_other_Gertrude</td>\n",
       "      <td>other</td>\n",
       "      <td>1.070888</td>\n",
       "      <td>0.786288</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gator_other_Gertrude</td>\n",
       "      <td>other</td>\n",
       "      <td>1.404810</td>\n",
       "      <td>0.946556</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hog_office_Bill</td>\n",
       "      <td>office</td>\n",
       "      <td>18.614739</td>\n",
       "      <td>10.711040</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hog_office_Bill</td>\n",
       "      <td>office</td>\n",
       "      <td>46.933820</td>\n",
       "      <td>29.432126</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hog_office_Bill</td>\n",
       "      <td>office</td>\n",
       "      <td>60.590159</td>\n",
       "      <td>34.006334</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hog_services_Kerrie</td>\n",
       "      <td>services</td>\n",
       "      <td>2.075842</td>\n",
       "      <td>1.448992</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hog_services_Kerrie</td>\n",
       "      <td>services</td>\n",
       "      <td>3.061034</td>\n",
       "      <td>2.400821</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hog_services_Kerrie</td>\n",
       "      <td>services</td>\n",
       "      <td>3.818607</td>\n",
       "      <td>2.549373</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Hog_warehouse_Porsha</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>1.958354</td>\n",
       "      <td>0.945058</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Hog_warehouse_Porsha</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>2.141530</td>\n",
       "      <td>1.150929</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Hog_warehouse_Porsha</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>2.911756</td>\n",
       "      <td>1.397100</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lamb_assembly_Bertie</td>\n",
       "      <td>assembly</td>\n",
       "      <td>17.967060</td>\n",
       "      <td>10.985065</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Lamb_assembly_Bertie</td>\n",
       "      <td>assembly</td>\n",
       "      <td>28.582859</td>\n",
       "      <td>22.667979</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Lamb_assembly_Bertie</td>\n",
       "      <td>assembly</td>\n",
       "      <td>48.756165</td>\n",
       "      <td>28.608991</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Lamb_industrial_Carla</td>\n",
       "      <td>industrial</td>\n",
       "      <td>45.353234</td>\n",
       "      <td>30.321867</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Lamb_industrial_Carla</td>\n",
       "      <td>industrial</td>\n",
       "      <td>43.913538</td>\n",
       "      <td>31.891065</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Lamb_industrial_Carla</td>\n",
       "      <td>industrial</td>\n",
       "      <td>53.681570</td>\n",
       "      <td>26.884887</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Peacock_lodging_Matthew</td>\n",
       "      <td>lodging</td>\n",
       "      <td>3.862391</td>\n",
       "      <td>2.968328</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Peacock_lodging_Matthew</td>\n",
       "      <td>lodging</td>\n",
       "      <td>4.518313</td>\n",
       "      <td>3.510761</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Peacock_lodging_Matthew</td>\n",
       "      <td>lodging</td>\n",
       "      <td>8.428719</td>\n",
       "      <td>6.074870</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Rat_public_Loretta</td>\n",
       "      <td>public</td>\n",
       "      <td>2.925038</td>\n",
       "      <td>1.817659</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Rat_public_Loretta</td>\n",
       "      <td>public</td>\n",
       "      <td>9.891990</td>\n",
       "      <td>6.787990</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Rat_public_Loretta</td>\n",
       "      <td>public</td>\n",
       "      <td>16.948175</td>\n",
       "      <td>12.082286</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Wolf_retail_Marcella</td>\n",
       "      <td>retail</td>\n",
       "      <td>1.187043</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Wolf_retail_Marcella</td>\n",
       "      <td>retail</td>\n",
       "      <td>1.857042</td>\n",
       "      <td>1.375036</td>\n",
       "      <td>daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wolf_retail_Marcella</td>\n",
       "      <td>retail</td>\n",
       "      <td>3.254573</td>\n",
       "      <td>2.155485</td>\n",
       "      <td>weekly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name building_id       RMSE        MAE horizon\n",
       "0        Bear_utility_Sidney     utility   1.157131   0.846614  hourly\n",
       "1        Bear_utility_Sidney     utility   1.255013   0.862390   daily\n",
       "2        Bear_utility_Sidney     utility   1.851878   1.167219  weekly\n",
       "3   Cockatoo_religion_Diedre    religion   1.475301   1.018945  hourly\n",
       "4   Cockatoo_religion_Diedre    religion   2.349360   1.820794   daily\n",
       "5   Cockatoo_religion_Diedre    religion   2.833513   1.958076  weekly\n",
       "6       Cockatoo_science_Rex     science   7.304536   5.529282  hourly\n",
       "7       Cockatoo_science_Rex     science  10.882962   7.975783   daily\n",
       "8       Cockatoo_science_Rex     science  12.667458   8.261340  weekly\n",
       "9     Eagle_education_Teresa   education   8.286079   5.855556  hourly\n",
       "10    Eagle_education_Teresa   education  11.534440   8.819952   daily\n",
       "11    Eagle_education_Teresa   education  14.939611  10.992661  weekly\n",
       "12      Eagle_health_Lucinda      health  24.377798  14.279867  hourly\n",
       "13      Eagle_health_Lucinda      health  40.084198  28.209270   daily\n",
       "14      Eagle_health_Lucinda      health  50.877437  35.407537  weekly\n",
       "15        Fox_food_Francesco        food   9.409997   6.518361  hourly\n",
       "16        Fox_food_Francesco        food  10.331829   7.536682   daily\n",
       "17        Fox_food_Francesco        food  18.896017  10.796031  weekly\n",
       "18        Fox_parking_Tommie     parking   2.536276   1.177474  hourly\n",
       "19        Fox_parking_Tommie     parking   3.155471   1.812750   daily\n",
       "20        Fox_parking_Tommie     parking   3.463871   1.573460  weekly\n",
       "21      Gator_other_Gertrude       other   0.232520   0.051120  hourly\n",
       "22      Gator_other_Gertrude       other   1.070888   0.786288   daily\n",
       "23      Gator_other_Gertrude       other   1.404810   0.946556  weekly\n",
       "24           Hog_office_Bill      office  18.614739  10.711040  hourly\n",
       "25           Hog_office_Bill      office  46.933820  29.432126   daily\n",
       "26           Hog_office_Bill      office  60.590159  34.006334  weekly\n",
       "27       Hog_services_Kerrie    services   2.075842   1.448992  hourly\n",
       "28       Hog_services_Kerrie    services   3.061034   2.400821   daily\n",
       "29       Hog_services_Kerrie    services   3.818607   2.549373  weekly\n",
       "30      Hog_warehouse_Porsha   warehouse   1.958354   0.945058  hourly\n",
       "31      Hog_warehouse_Porsha   warehouse   2.141530   1.150929   daily\n",
       "32      Hog_warehouse_Porsha   warehouse   2.911756   1.397100  weekly\n",
       "33      Lamb_assembly_Bertie    assembly  17.967060  10.985065  hourly\n",
       "34      Lamb_assembly_Bertie    assembly  28.582859  22.667979   daily\n",
       "35      Lamb_assembly_Bertie    assembly  48.756165  28.608991  weekly\n",
       "36     Lamb_industrial_Carla  industrial  45.353234  30.321867  hourly\n",
       "37     Lamb_industrial_Carla  industrial  43.913538  31.891065   daily\n",
       "38     Lamb_industrial_Carla  industrial  53.681570  26.884887  weekly\n",
       "39   Peacock_lodging_Matthew     lodging   3.862391   2.968328  hourly\n",
       "40   Peacock_lodging_Matthew     lodging   4.518313   3.510761   daily\n",
       "41   Peacock_lodging_Matthew     lodging   8.428719   6.074870  weekly\n",
       "42        Rat_public_Loretta      public   2.925038   1.817659  hourly\n",
       "43        Rat_public_Loretta      public   9.891990   6.787990   daily\n",
       "44        Rat_public_Loretta      public  16.948175  12.082286  weekly\n",
       "45      Wolf_retail_Marcella      retail   1.187043   0.793767  hourly\n",
       "46      Wolf_retail_Marcella      retail   1.857042   1.375036   daily\n",
       "47      Wolf_retail_Marcella      retail   3.254573   2.155485  weekly"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_meters_path = \"..\\\\data\\\\\"\n",
    "\n",
    "# files in directory\n",
    "files = glob(cleaned_meters_path + \"*.csv\")\n",
    "\n",
    "\n",
    "benchmark = pd.read_csv(files[0]) # load the dataset\n",
    "\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a78fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingNames = benchmark['name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f1a0e",
   "metadata": {},
   "source": [
    "# Specify Constants\n",
    "\n",
    "\n",
    "In order to obtain some of the constants we need to load a representative dataframe of one building for one meter_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182d4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the first \n",
    "building_name = buildingNames[0]\n",
    "representative_df = complete_data_cleaned.loc[(complete_data_cleaned['building_id'] == building_name)\n",
    "                                               & (complete_data_cleaned['meter'] == 'electricity_cleaned')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d24e3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.3\n",
    "DATAPOINS_PER_BUILDING_AND_METER_TYPE = representative_df.shape[0]\n",
    "SPLIT_INDEX = int(DATAPOINS_PER_BUILDING_AND_METER_TYPE * (1 - TRAIN_TEST_SPLIT))\n",
    "SPLIT_TIMESTAMP =  representative_df.iloc[SPLIT_INDEX]['timestamp']\n",
    "TRAIN_DIMENSIONS = 5\n",
    "SAMPLE_SIZE = 2000\n",
    "DATAPOINTS_ONE_WEEK = 24 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b61c91b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>meter</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dayOfWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21561576</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>53.0325</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21561577</th>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>54.5325</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21561578</th>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>55.1525</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21561579</th>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>54.0900</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21561580</th>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>53.9325</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21573851</th>\n",
       "      <td>2017-05-26 11:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>62.7850</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21573852</th>\n",
       "      <td>2017-05-26 12:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>61.9200</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21573853</th>\n",
       "      <td>2017-05-26 13:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>61.1675</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21573854</th>\n",
       "      <td>2017-05-26 14:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>57.9525</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21573855</th>\n",
       "      <td>2017-05-26 15:00:00</td>\n",
       "      <td>Bear_utility_Sidney</td>\n",
       "      <td>54.1050</td>\n",
       "      <td>electricity_cleaned</td>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12280 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp          building_id  meter_reading  \\\n",
       "21561576 2016-01-01 00:00:00  Bear_utility_Sidney        53.0325   \n",
       "21561577 2016-01-01 01:00:00  Bear_utility_Sidney        54.5325   \n",
       "21561578 2016-01-01 02:00:00  Bear_utility_Sidney        55.1525   \n",
       "21561579 2016-01-01 03:00:00  Bear_utility_Sidney        54.0900   \n",
       "21561580 2016-01-01 04:00:00  Bear_utility_Sidney        53.9325   \n",
       "...                      ...                  ...            ...   \n",
       "21573851 2017-05-26 11:00:00  Bear_utility_Sidney        62.7850   \n",
       "21573852 2017-05-26 12:00:00  Bear_utility_Sidney        61.9200   \n",
       "21573853 2017-05-26 13:00:00  Bear_utility_Sidney        61.1675   \n",
       "21573854 2017-05-26 14:00:00  Bear_utility_Sidney        57.9525   \n",
       "21573855 2017-05-26 15:00:00  Bear_utility_Sidney        54.1050   \n",
       "\n",
       "                        meter        date  year  month  dayOfWeek  \n",
       "21561576  electricity_cleaned  2016-01-01  2016      1          4  \n",
       "21561577  electricity_cleaned  2016-01-01  2016      1          4  \n",
       "21561578  electricity_cleaned  2016-01-01  2016      1          4  \n",
       "21561579  electricity_cleaned  2016-01-01  2016      1          4  \n",
       "21561580  electricity_cleaned  2016-01-01  2016      1          4  \n",
       "...                       ...         ...   ...    ...        ...  \n",
       "21573851  electricity_cleaned  2017-05-26  2017      5          4  \n",
       "21573852  electricity_cleaned  2017-05-26  2017      5          4  \n",
       "21573853  electricity_cleaned  2017-05-26  2017      5          4  \n",
       "21573854  electricity_cleaned  2017-05-26  2017      5          4  \n",
       "21573855  electricity_cleaned  2017-05-26  2017      5          4  \n",
       "\n",
       "[12280 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representative_df[:SPLIT_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469aace4",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d986cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normalizes the data to a Gaussian distribution using quantiles.\n",
    "\"\"\"\n",
    "def normalizeToGaussian(arr, mode=\"mean\"):\n",
    "    n = len(arr)\n",
    "    perc = percentileofscore\n",
    "    arr_ = arr.copy()[~np.isnan(arr)]\n",
    "    out = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if not np.isnan(arr[i]):\n",
    "            out[i] = norm.ppf(perc(arr_, arr[i], mode) / 100.)\n",
    "        else:\n",
    "            out[i] = np.nan\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d5d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that transforms the Gaussian normalisation back to percentiles of scores\n",
    "'''\n",
    "def gaussianToCdf(arr):\n",
    "    n = len(arr)\n",
    "    out = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if not np.isnan(arr[i]):\n",
    "            out[i] = norm.cdf(arr[i])\n",
    "        else:\n",
    "            out[i] = np.nan\n",
    "            \n",
    "    out = out\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0725cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformIntoOriginalRange(arr, original):\n",
    "\n",
    "    \n",
    "    original_min = np.min(original)\n",
    "    original_max = np.max(original)\n",
    "    original_mean = np.nanmean(original) \n",
    "    original_std = np.nanstd(original)\n",
    "\n",
    "    # Calculate the range, mean, and standard deviation of the predictions\n",
    "    arr_range = np.max(arr) - np.min(arr)\n",
    "    arr_mean = np.mean(arr)\n",
    "    arr_std = np.std(arr)\n",
    "\n",
    "    # Rescale the predictions based on the original signal's range and statistics\n",
    "    rescaled = (\n",
    "        (arr - arr_mean) * (original_std / arr_std) + original_mean\n",
    "    )\n",
    "\n",
    "    # Adjust the rescaled predictions to fit within the original signal's range\n",
    "    min_diff = original_min - np.min(rescaled)\n",
    "    max_diff = np.max(rescaled) - original_max\n",
    "    if min_diff > 0:\n",
    "        rescaled -= min_diff\n",
    "    elif max_diff > 0:\n",
    "        rescaled -= max_diff\n",
    " \n",
    "    return rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43572cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Replaces missing values in the dataframe with the mean of the month for all years from which we have data\n",
    "@param dataframe\n",
    "@returns numpy array\n",
    "'''\n",
    "def averageNaNs (df, field):\n",
    "    mean_df = df.groupby(['month']).mean()\n",
    "    if (len(mean_df.index) == 12) :\n",
    "        averaged_mean = df[field].copy().fillna(\n",
    "                                    df['month'].map({1: mean_df[field][1] , 2: mean_df[field][2], 3:mean_df[field][3],\n",
    "                                                     4: mean_df[field][4] , 5: mean_df[field][5], 6:mean_df[field][6], \n",
    "                                                     7: mean_df[field][7] , 8: mean_df[field][8], 9:mean_df[field][9],\n",
    "                                                     10: mean_df[field][10] , 11: mean_df[field][11], 12:mean_df[field][12]}))\n",
    "    else :\n",
    "        averaged_mean = df[field].copy().fillna(\n",
    "                                    df['month'].map({5: mean_df[field][5], 6:mean_df[field][6], \n",
    "                                                     7: mean_df[field][7] , 8: mean_df[field][8], 9:mean_df[field][9],\n",
    "                                                     10: mean_df[field][10] , 11: mean_df[field][11], 12:mean_df[field][12]}))\n",
    "    averaged_numpy = averaged_mean.to_numpy()\n",
    "    return averaged_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "242e16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train a Gaussian Process model based on the training data\n",
    "@param two numpy arrays of same dimensionality\n",
    "@returns numpy array with lenght NUMBER_OF_DIMENSIONS, containing the indices of the most correlated stations\n",
    "'''\n",
    "def trainGP (X_train, Y_train):\n",
    "    \n",
    "    # Shape of training data needs to be consistent \n",
    "    assert X_train.shape[0] == Y_train.shape[0]\n",
    "    \n",
    "    kernel = GPy.kern.RBF(input_dim=X_train.shape[1])\n",
    "    model = GPy.models.GPRegression(X_train, Y_train, kernel)\n",
    "    model.optimize(messages=True)\n",
    "    model.optimize_restarts(num_restarts=5)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b145a",
   "metadata": {
    "direction": "ltr"
   },
   "source": [
    "# So far unused helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33bd4009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Computation of Spearmas Rank Correlation\n",
    "@param  target_df: a pandas.dataframe (target) for wich we want to calculate the correlation matrix \n",
    "        dataset: a list of pandas dataframes that build our base, as we check for the correlation between dataframe and each df in the truncated dataset\n",
    "@returns spearmans_matrix: a numpy.ndarray that contains the correlation value as first entry in each row and the corresponding p-value as a second element in each row. \n",
    "                            the order of stations in the matrix is consistent to the order in the dataset:\n",
    "'''\n",
    "def spearmansCorrelation (target, possible_features): \n",
    "    y_target = target['meter_reading']\n",
    "    spearmans_matrix = np.zeros((len(possible_features), 2))\n",
    "    for index, feature in enumerate(possible_features): \n",
    "        if index % 100 == 0 : print(index)\n",
    "        if ((target.shape[0] == feature.shape[0]) \n",
    "            and not ((feature['meter_reading'].isnull().sum()/feature.shape[0]) > 0.3)):\n",
    "            correlation, _ = stats.spearmanr(y_target, feature['meter_reading'], nan_policy='omit')\n",
    "            spearmans_matrix[index] = correlation, 0    \n",
    "\n",
    "        else: \n",
    "            spearmans_matrix[index] = 0,100 \n",
    "            continue\n",
    "            \n",
    "    return spearmans_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a802d",
   "metadata": {},
   "source": [
    "### Load dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45058acd",
   "metadata": {},
   "source": [
    "### Transform into Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8681658",
   "metadata": {},
   "source": [
    "### Split into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0f5a9",
   "metadata": {},
   "source": [
    "## Specify and Train Gaussian Process model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba1404",
   "metadata": {},
   "source": [
    "###### Plot the results given driectly from the GP\n",
    "\n",
    "Note as we previously transformed the data to be normal distributed, the data we obtain here is as well transformed <br>\n",
    "In order to reconstruct the original data we need to transform it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "472dd0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nx = np.linspace(0, np.size(mean), np.size(mean))\\n\\nplt.figure(figsize=(12,4))\\nplt.xlabel('Hours')\\nplt.ylabel('Energy_consumption')\\n\\nplt.plot(x, mean, label='Prediction')\\nplt.plot(x, test_meter[0:200], label='Ground truth')\\nplt.fill_between(x, (mean - var).flatten(), (mean + var).flatten(), color='lightblue', alpha=0.5, label='Variance')\\nplt.legend()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x = np.linspace(0, np.size(mean), np.size(mean))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Energy_consumption')\n",
    "\n",
    "plt.plot(x, mean, label='Prediction')\n",
    "plt.plot(x, test_meter[0:200], label='Ground truth')\n",
    "plt.fill_between(x, (mean - var).flatten(), (mean + var).flatten(), color='lightblue', alpha=0.5, label='Variance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad919d",
   "metadata": {},
   "source": [
    "## Convert the results back and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8beee30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprediction_transformed = transformIntoOriginalRange(mean.flatten(), original_data_train)\\nvariance_transformed = transformIntoOriginalRange(var.flatten(), original_data_train)\\noriginal_test_transformed = transformIntoOriginalRange(original_data_test[0:200], original_data_train)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "prediction_transformed = transformIntoOriginalRange(mean.flatten(), original_data_train)\n",
    "variance_transformed = transformIntoOriginalRange(var.flatten(), original_data_train)\n",
    "original_test_transformed = transformIntoOriginalRange(original_data_test[0:200], original_data_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27521de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nx = np.linspace(0, np.size(prediction_transformed), np.size(prediction_transformed))\\n\\nplt.figure(figsize=(12,4))\\nplt.xlabel('Hours')\\nplt.ylabel('Energy_consumption')\\n\\nplt.plot(x, prediction_transformed, label='Prediction')\\nplt.plot(x, original_data_test[0:200], label='Ground truth')\\nplt.legend()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x = np.linspace(0, np.size(prediction_transformed), np.size(prediction_transformed))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Energy_consumption')\n",
    "\n",
    "plt.plot(x, prediction_transformed, label='Prediction')\n",
    "plt.plot(x, original_data_test[0:200], label='Ground truth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249da91",
   "metadata": {},
   "source": [
    "# Error computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c962fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_metrics(y_target, prediction):\n",
    "    assert (y_target.shape == prediction.shape)\n",
    "    mse = mean_squared_error(y_target, prediction_transformed)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_target, prediction_transformed)\n",
    "\n",
    "    \n",
    "    return mse, rmse, mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb57083",
   "metadata": {},
   "source": [
    "## Iteration over all buildings we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0c488f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For our given dataset we \n",
    "'''\n",
    "def getDfsOfOtherMeterTypes(df): \n",
    "    dfs_meter_types = []\n",
    "    meter_types = df['meter'].unique()\n",
    "\n",
    "    for meterType in meter_types:\n",
    "        if meterType == 'electricity_cleaned':\n",
    "            continue\n",
    "        \n",
    "        dfs_meter_types.append(df.loc[df['meter'] == meterType]) \n",
    "        \n",
    "    return dfs_meter_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03ebdd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_ids = complete_data_cleaned['building_id'].unique()\n",
    "meter_types = complete_data_cleaned['meter'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c08a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function not needed\n",
    "\n",
    "def getAllMeterDfsExceptTarget(building_id): \n",
    "    dfs = []\n",
    "    for id in building_ids:\n",
    "        print(building_id)\n",
    "        building = complete_data_cleaned.loc[complete_data_cleaned['building_id'] == id]\n",
    "        for meter_type in building['meter'].unique(): \n",
    "            # We do not want to contain our target into this list of dataframes\n",
    "            if ((id == building_id) and (meter_type == 'electricity_cleaned')):\n",
    "                continue\n",
    "            \n",
    "            building_meter = building.loc[building['meter'] == meter_type]\n",
    "            dfs.append(building_meter) \n",
    "        \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77cd82e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1608195383.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    complete_data_grouped =\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# This cell might take some time to finish\n",
    "\n",
    "# Group the DataFrame by 'building_id' and 'meter_type'\n",
    "complete_data_grouped = complete_data_cleaned.groupby(['building_id', 'meter'])\n",
    "\n",
    "# Initialize an empty list to store the smaller DataFrames\n",
    "dfs = []\n",
    "df_keys = []\n",
    "\n",
    "# Iterate over the groups and create smaller DataFrames\n",
    "for group_key, group in complete_data_grouped:\n",
    "    df_keys.append(group_key)\n",
    "    dfs.append(group.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bccea9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = np.array(df_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "359bdebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For given dataset (list of pd.Dataframes) we compute a subsample of those dataframes such that we obtain a list of dfs\n",
    "# that do not fit the given building_name and are not of meter_type 'electricity_cleaned'\n",
    "# Also we obtain the index/position of the dataframe that matches the building_id and is of type electricity_cleaned\n",
    "def get_all_features(dataset, building_name):\n",
    "    key_to_exclude = [building_name, 'electricity_cleaned']\n",
    "    features = []\n",
    "    index_of_df = np.nan\n",
    "    for index, key in enumerate(group_keys):\n",
    "        # as the key composes of two elements, the buildling_id and the meter_type we compare two arrays of strings, therefore we need the .all() \n",
    "        if(key == key_to_exclude).all():\n",
    "            index_of_df = index\n",
    "            continue\n",
    "        features.append(dataset[index])\n",
    "            \n",
    "    return features, index_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f356dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataframe into train and test data according to a split_date\n",
    "def split_df_to_train_and_test(df, split_date):\n",
    "    df_train = df.loc[df['timestamp'] < split_date].copy().reset_index(drop=True) \n",
    "    df_test  = df.loc[df['timestamp'] >= split_date].copy().reset_index(drop=True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dffbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = []\n",
    "dfs_test = []\n",
    "\n",
    "for df in dfs: \n",
    "    \n",
    "    temp_train_df, temp_test_df = split_df_to_train_and_test(df, SPLIT_TIMESTAMP)\n",
    "    dfs_train.append(temp_train_df)\n",
    "    dfs_test.append(temp_test_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "def44fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def sort_according_to_highest_correlation(spearmans_matrix):\n",
    "    correlation_values = spearmans_matrix[:,0]\n",
    "    clean_correlation_values = np.nan_to_num(correlation_values, 0)\n",
    "    clean_abs_correlation_values = np.abs(clean_correlation_values)\n",
    "    sorted_indices = np.argsort(clean_abs_correlation_values)[::-1]\n",
    "    \n",
    "    return sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ac7af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a dataset (list of pd.Dataframes) and a list of sorted indice\n",
    "def transform_highest_correlated_dfs_to_training_data(list_of_dfs ,indices):\n",
    "    transformed_features = []\n",
    "    for ind in indices[:TRAIN_DIMENSIONS]:\n",
    "        temp_df = list_of_dfs[ind]\n",
    "        avg_temp_df = averageNaNs(temp_df, 'meter_reading')\n",
    "        transformed_features.append(normalizeToGaussian(avg_temp_df))\n",
    "    \n",
    "    return np.column_stack((transformed_features))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6c3c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X_train, y_train, num_samples=SAMPLE_SIZE, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    indices = np.random.choice(len(X_train), size=num_samples, replace=False)\n",
    "    X_samples = X_train[indices]\n",
    "    y_samples = y_train[indices]\n",
    "\n",
    "    return X_samples, y_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17341672",
   "metadata": {},
   "source": [
    "# ToDos:\n",
    "\n",
    "test if pearsons correlation is faster?\n",
    "Check if it makes sense to compute once and for all this huge correlation matrix for every df with every other df?\n",
    "One could compute this in an extra notebook, save it and then one could load it here and would not need to perform this super expensive computation for every dataframe\n",
    "\n",
    "\n",
    "Not even needed to have all the correlation but only the ones of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921895c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bear_utility_Sidney\n",
      "loading all other features ...\n",
      "all other features loaded\n",
      "compute spearmans matrix ...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\marku\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:4484: SpearmanRConstantInputWarning:An input array is constant; the correlation coefficient is not defined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name in buildingNames[0:1]: \n",
    "    print(name)\n",
    "    \n",
    "    # Get dataframe that matches our building_id\n",
    "    df = complete_data_cleaned.loc[complete_data_cleaned['building_id']  == name]\n",
    "    \n",
    "    # For later reconstruction we split the electricity meter reading into train and test set\n",
    "    y_train_df, y_test_df = split_df_to_train_and_test(df, SPLIT_TIMESTAMP)\n",
    "    \n",
    "    print(\"loading all other features ...\")\n",
    "    # Load all other features like water consumption, gas ....\n",
    "    features_train, own_index = get_all_features(dfs_train, name)\n",
    "\n",
    "    print(\"all other features loaded\")\n",
    "    \n",
    "    # OPTIONAL\n",
    "    # 1. Load the weather data for the location\n",
    "    #siteName = name.split('_')[0]\n",
    "    #weather = weather.loc[weather['site_id'] == siteName]\n",
    "    # 2. Load the water data\n",
    "    \n",
    "    print(\"compute spearmans matrix ...\")\n",
    "    # Compute the spearmans correlation between all those features in order to pick the most relevant ones\n",
    "    spearmans_matrix = spearmansCorrelation(y_train_df, features_train)\n",
    "    print(\"spearmans matrix comptued\", spearmans_matrix)\n",
    "\n",
    "    \n",
    "    # Sort according to highest correlation\n",
    "    sorted_indices = sort_according_to_highest_correlation(spearmans_matrix)\n",
    "    print(\"sorted indices: \", sorted_indices)\n",
    "    \n",
    "    \n",
    "    print(\"transforming highest correlated dfs\")\n",
    "   \n",
    "    # Bring data into format to fit as GP Regression input (normal distribution)\n",
    "    X_train = transform_highest_correlated_dfs_to_training_data(dfs_train, sorted_indices)\n",
    "    X_test = transform_highest_correlated_dfs_to_training_data(dfs_test, sorted_indices)\n",
    "\n",
    "    print(\"X_train.shape\", X_train.shape)\n",
    "    print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "    Y_train = normalizeToGaussian(averageNaNs(y_train_df, 'meter_reading'))\n",
    "    Y_test = normalizeToGaussian(averageNaNs(y_test_df, 'meter_reading'))\n",
    "    \n",
    "    \n",
    "    print(\"Y_train.shape\", Y_train.shape)\n",
    "    print(\"Y_test.shape\", Y_test.shape)\n",
    "\n",
    "    sampled_X_train, sampled_Y_train = random_sampling(X_train, Y_train)\n",
    "    \n",
    "    print(\"sampled_X_train.shape\", sampled_X_train.shape)\n",
    "    print(\"sampled_Y_train.shape\", sampled_Y_train.shape)\n",
    "        \n",
    "    # Randomly sample from the datapoints, as we cannot train on the whole dimensions\n",
    "    \n",
    "    print(\"Training model ....\")\n",
    "    # Train the model \n",
    "    model = trainGP (sampled_X_train, sampled_Y_train.reshape(-1,1))\n",
    "    \n",
    "    print(\"predicting ...\")\n",
    "    # Make predictions with the trained model for one week\n",
    "    mean, var = model.predict(X_test[0:DATAPOINTS_ONE_WEEK])\n",
    "    \n",
    "    #Compute the error values for each timesoan hour, day, week\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"It finished!\")\n",
    "assert(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc539d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"transforming highest correlated dfs\")\n",
    "# Bring data into format to fit as GP Regression input (normal distribution)\n",
    "X_train = transform_highest_correlated_dfs_to_training_data(dfs_train, sorted_indices)\n",
    "X_test = transform_highest_correlated_dfs_to_training_data(dfs_test, sorted_indices)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "Y_train = normalizeToGaussian(averageNaNs(y_train_df, 'meter_reading'))\n",
    "Y_test = normalizeToGaussian(averageNaNs(y_test_df, 'meter_reading'))\n",
    "\n",
    "\n",
    "print(\"Y_train.shape\", Y_train.shape)\n",
    "print(\"Y_test.shape\", Y_test.shape)\n",
    "\n",
    "sampled_X_train, sampled_Y_train = random_sampling(X_train, Y_train)\n",
    "\n",
    "print(\"sampled_X_train.shape\", sampled_X_train.shape)\n",
    "print(\"sampled_Y_train.shape\", sampled_Y_train.shape)\n",
    "\n",
    "# Randomly sample from the datapoints, as we cannot train on the whole dimensions\n",
    "\n",
    "print(\"Training model ....\")\n",
    "# Train the model \n",
    "model = trainGP (sampled_X_train, sampled_Y_train.reshape(-1,1))\n",
    "\n",
    "print(\"predicting ...\")\n",
    "# Make predictions with the trained model for one week\n",
    "mean, var = model.predict(X_test[0:200])\n",
    "\n",
    "#Compute the error values for each timesoan hour, day, week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca892ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, np.size(mean), np.size(mean))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Energy_consumption')\n",
    "\n",
    "plt.plot(x, mean, label='Prediction')\n",
    "plt.plot(x, test_meter[0:200], label='Ground truth')\n",
    "plt.fill_between(x, (mean - var).flatten(), (mean + var).flatten(), color='lightblue', alpha=0.5, label='Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f01bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_transformed = transformIntoOriginalRange(mean.flatten(), y_train_df['meter_reading'])\n",
    "variance_transformed = transformIntoOriginalRange(var.flatten(), y_train_df['meter_reading'])\n",
    "original_test_transformed = transformIntoOriginalRange(y_test_df['meter_reading'][0:200], y_train_df['meter_reading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test_df['meter_reading'][0:200], prediction_transformed)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_df['meter_reading'][0:200], prediction_transformed)\n",
    "\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cec87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "direction": "ltr",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
